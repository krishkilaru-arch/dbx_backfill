{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "414a3621-59c3-40c0-a26e-dacc2ca8ad0b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Backfill Demo: Setup Notebook\n",
    "\n",
    "This notebook sets up all required tables for demonstrating Databricks backfilling capabilities.\n",
    "\n",
    "## Tables Created:\n",
    "1. **Calendar Table** - Business day calendar (2020-2026)\n",
    "2. **Source Table** - Historical data with business dates\n",
    "3. **Destination Table** - Partitioned target table for processed data\n",
    "4. **Backfill Log Table** - Tracks backfill operations and their status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ad301d8-a216-4258-812f-e227b9d20b6a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\uD83D\uDCC1 Catalog: demos\n\uD83D\uDCC2 Schema: backfill_demo\n\uD83D\uDCCA Tables:\n  • Source: demos.backfill_demo.source_data\n  • Destination: demos.backfill_demo.destination_data\n  • Calendar: demos.backfill_demo.calendar\n  • Backfill Log: demos.backfill_demo.backfill_log\n"
     ]
    }
   ],
   "source": [
    "# Load Configuration\n",
    "import sys\n",
    "\n",
    "# Get current notebook path dynamically\n",
    "notebook_path = dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get()\n",
    "workspace_path = f\"/Workspace{notebook_path}\"\n",
    "base_path = workspace_path.rsplit('/', 1)[0]\n",
    "\n",
    "# Add to sys.path for importing config\n",
    "sys.path.append(base_path)\n",
    "\n",
    "from config import CATALOG, SCHEMA, SOURCE_TABLE, DEST_TABLE, CALENDAR_TABLE, BACKFILL_LOG_TABLE, print_config\n",
    "\n",
    "# Display configuration\n",
    "print_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a06b7a8-83b7-4089-bc8c-c1f2af618db8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Catalog 'demos' ready\n✓ Schema 'demos.backfill_demo' ready\n"
     ]
    }
   ],
   "source": [
    "# Create Catalog and Schema if they don't exist\n",
    "spark.sql(f\"CREATE CATALOG IF NOT EXISTS {CATALOG}\")\n",
    "print(f\"✓ Catalog '{CATALOG}' ready\")\n",
    "\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {CATALOG}.{SCHEMA}\")\n",
    "print(f\"✓ Schema '{CATALOG}.{SCHEMA}' ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9b02fce-0d30-4f39-b6f4-ffdf09a48b62",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Created table: demos.backfill_demo.calendar\n✓ Created table: demos.backfill_demo.source_data\n✓ Created table: demos.backfill_demo.destination_data\n✓ Created table: demos.backfill_demo.backfill_log\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Create DDLs - Managed Tables\n",
    "\n",
    "# 3.1 Calendar Table\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {CALENDAR_TABLE} (\n",
    "        calendar_date DATE NOT NULL,\n",
    "        calendar_year INT,\n",
    "        quarter_of_the_year INT,\n",
    "        month_of_the_year INT,\n",
    "        week_of_the_month INT,\n",
    "        us_business_or_holiday_flag STRING,\n",
    "        global_business_or_holiday_flag STRING\n",
    "    )\n",
    "    USING DELTA\n",
    "\"\"\")\n",
    "print(f\"✓ Created table: {CALENDAR_TABLE}\")\n",
    "\n",
    "# 3.2 Source Data Table\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {SOURCE_TABLE} (\n",
    "        position_date DATE,\n",
    "        id BIGINT,\n",
    "        value DOUBLE,\n",
    "        category STRING\n",
    "    )\n",
    "    USING DELTA\n",
    "\"\"\")\n",
    "print(f\"✓ Created table: {SOURCE_TABLE}\")\n",
    "\n",
    "# 3.3 Destination Data Table (Partitioned)\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {DEST_TABLE} (\n",
    "        position_date DATE,\n",
    "        id BIGINT,\n",
    "        value DOUBLE,\n",
    "        category STRING\n",
    "    )\n",
    "    USING DELTA\n",
    "    PARTITIONED BY (position_date)\n",
    "\"\"\")\n",
    "print(f\"✓ Created table: {DEST_TABLE}\")\n",
    "\n",
    "# 3.4 Backfill Log Table\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {BACKFILL_LOG_TABLE} (\n",
    "        run_id STRING NOT NULL,\n",
    "        position_date DATE,\n",
    "        job_name STRING,\n",
    "        job_id STRING,\n",
    "        status STRING,\n",
    "        start_time TIMESTAMP,\n",
    "        end_time TIMESTAMP,\n",
    "        backfill_job_id STRING,\n",
    "        error_message STRING,\n",
    "        creator_user_name STRING,\n",
    "        run_page_url STRING,\n",
    "        retry_metadata STRING COMMENT 'JSON: {is_retry, original_run_id, retry_count, retry_triggered_by}'\n",
    "    )\n",
    "    USING DELTA\n",
    "\"\"\")\n",
    "print(f\"✓ Created table: {BACKFILL_LOG_TABLE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2a11a231-357e-4828-9b37-09416f889dbb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Data Generation\n",
    "\n",
    "We'll populate the tables with sample data using efficient Spark operations (no Python loops!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "beba9ef3-0710-4a68-bd8a-8431438b8e81",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Populated demos.backfill_demo.source_data\n  └─ 4,940 rows across 494 business days\n"
     ]
    }
   ],
   "source": [
    "# Populate Source Table with Sample Data\n",
    "import numpy as np\n",
    "from pyspark.sql.types import StructType, StructField, DateType, LongType, DoubleType, StringType\n",
    "from pyspark.sql import functions as F\n",
    "from datetime import datetime\n",
    "\n",
    "# Set seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate business dates for 2024-2025\n",
    "start_date = datetime(2024, 1, 1)\n",
    "end_date = datetime(2025, 11, 20)\n",
    "\n",
    "# Use Spark SQL to generate dates efficiently\n",
    "dates_df = spark.sql(f\"\"\"\n",
    "    SELECT explode(sequence(to_date('{start_date.date()}'), to_date('{end_date.date()}'), interval 1 day)) as position_date\n",
    "\"\"\").filter(F.dayofweek(F.col(\"position_date\")).between(2, 6))  # Monday=2 to Friday=6\n",
    "\n",
    "# Generate 10 records per date with cross join\n",
    "records_df = spark.range(0, 10).selectExpr(\"id as id\")\n",
    "\n",
    "# Create sample data using cross join\n",
    "sample_data = dates_df.crossJoin(records_df).select(\n",
    "    F.col(\"position_date\"),\n",
    "    F.col(\"id\"),\n",
    "    (F.rand(seed=42) * 100).alias(\"value\"),\n",
    "    F.array(F.lit(\"A\"), F.lit(\"B\"), F.lit(\"C\"))[(F.rand(seed=43) * 3).cast(\"int\")].alias(\"category\")\n",
    ")\n",
    "\n",
    "# Write to source table\n",
    "sample_data.write.format(\"delta\").mode(\"overwrite\").saveAsTable(SOURCE_TABLE)\n",
    "\n",
    "row_count = sample_data.count()\n",
    "date_count = dates_df.count()\n",
    "print(f\"✓ Populated {SOURCE_TABLE}\")\n",
    "print(f\"  └─ {row_count:,} rows across {date_count} business days\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a82086be-b9d9-427d-8a92-3a1ae45bbdea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\uD83D\uDCC5 Loaded 76 US federal holidays (2020-2026)\n✓ Populated demos.backfill_demo.calendar\n  └─ Total days: 2,557 (2020-2026)\n  └─ Business days: 1,751\n  └─ Holidays/Weekends: 806\n\n\uD83D\uDCCB Sample US Federal Holidays Marked:\n  • 2020-01-01\n  • 2020-01-20\n  • 2020-02-17\n  • 2020-05-25\n  • 2020-07-03\n"
     ]
    }
   ],
   "source": [
    "# Populate Calendar Table (2020-2026) with Real US Holidays\n",
    "from pyspark.sql import functions as F\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from pandas.tseries.holiday import USFederalHolidayCalendar\n",
    "\n",
    "# Generate date range from 2020 to 2026\n",
    "start_date = datetime(2020, 1, 1)\n",
    "end_date = datetime(2026, 12, 31)\n",
    "\n",
    "# Get US federal holidays using pandas\n",
    "cal = USFederalHolidayCalendar()\n",
    "us_holidays = cal.holidays(start=start_date, end=end_date)\n",
    "us_holiday_dates = set(us_holidays.date)\n",
    "print(f\"\uD83D\uDCC5 Loaded {len(us_holiday_dates)} US federal holidays (2020-2026)\")\n",
    "\n",
    "# Generate all dates using Spark SQL\n",
    "all_dates_df = spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        explode(sequence(to_date('{start_date.date()}'), to_date('{end_date.date()}'), interval 1 day)) as calendar_date\n",
    "\"\"\")\n",
    "\n",
    "# Convert to pandas for holiday checking, then back to Spark\n",
    "dates_pd = all_dates_df.toPandas()\n",
    "\n",
    "# Ensure calendar_date is datetime type\n",
    "dates_pd['calendar_date'] = pd.to_datetime(dates_pd['calendar_date'])\n",
    "\n",
    "# Add calendar columns\n",
    "dates_pd['calendar_year'] = dates_pd['calendar_date'].dt.year\n",
    "dates_pd['quarter_of_the_year'] = dates_pd['calendar_date'].dt.quarter\n",
    "dates_pd['month_of_the_year'] = dates_pd['calendar_date'].dt.month\n",
    "dates_pd['week_of_the_month'] = ((dates_pd['calendar_date'].dt.day - 1) // 7) + 1\n",
    "\n",
    "# Determine business day vs holiday\n",
    "# B = Business day (Monday-Friday, not a US federal holiday)\n",
    "# H = Holiday (Weekend or US federal holiday)\n",
    "def is_business_day(dt):\n",
    "    # Weekend check\n",
    "    if dt.weekday() >= 5:  # Saturday=5, Sunday=6\n",
    "        return 'H'\n",
    "    # US holiday check\n",
    "    if dt.date() in us_holiday_dates:\n",
    "        return 'H'\n",
    "    return 'B'\n",
    "\n",
    "dates_pd['us_business_or_holiday_flag'] = dates_pd['calendar_date'].apply(is_business_day)\n",
    "dates_pd['global_business_or_holiday_flag'] = dates_pd['us_business_or_holiday_flag']  # Same for this demo\n",
    "\n",
    "# Convert calendar_date to date (not datetime) for Pandas\n",
    "dates_pd['calendar_date'] = dates_pd['calendar_date'].dt.date\n",
    "\n",
    "# Convert back to Spark DataFrame with explicit date type\n",
    "calendar_df = spark.createDataFrame(dates_pd)\n",
    "\n",
    "# Write to calendar table with overwriteSchema to handle any schema changes\n",
    "calendar_df.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(CALENDAR_TABLE)\n",
    "\n",
    "# Get statistics\n",
    "stats = calendar_df.groupBy(\"us_business_or_holiday_flag\").count().collect()\n",
    "total_days = sum([row['count'] for row in stats])\n",
    "business_days = [row['count'] for row in stats if row['us_business_or_holiday_flag'] == 'B'][0]\n",
    "holidays_count = total_days - business_days\n",
    "\n",
    "print(f\"✓ Populated {CALENDAR_TABLE}\")\n",
    "print(f\"  └─ Total days: {total_days:,} (2020-2026)\")\n",
    "print(f\"  └─ Business days: {business_days:,}\")\n",
    "print(f\"  └─ Holidays/Weekends: {holidays_count:,}\")\n",
    "print(f\"\\n\uD83D\uDCCB Sample US Federal Holidays Marked:\")\n",
    "for dt in sorted(list(us_holiday_dates))[:5]:\n",
    "    print(f\"  • {dt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc569d75-ad1f-47ab-9a46-5e38c2cacb85",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n\uD83D\uDCCA Quick Data Preview:\n\nCalendar Table:\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>calendar_date</th><th>calendar_year</th><th>quarter_of_the_year</th><th>month_of_the_year</th><th>week_of_the_month</th><th>us_business_or_holiday_flag</th><th>global_business_or_holiday_flag</th></tr></thead><tbody><tr><td>2025-01-15</td><td>2025</td><td>1</td><td>1</td><td>3</td><td>B</td><td>B</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "2025-01-15",
         2025,
         1,
         1,
         3,
         "B",
         "B"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "calendar_date",
         "type": "\"date\""
        },
        {
         "metadata": "{}",
         "name": "calendar_year",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "quarter_of_the_year",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "month_of_the_year",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "week_of_the_month",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "us_business_or_holiday_flag",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "global_business_or_holiday_flag",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Data Table:\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>position_date</th><th>id</th><th>value</th><th>category</th></tr></thead><tbody><tr><td>2025-01-15</td><td>3</td><td>73.44850394468924</td><td>B</td></tr><tr><td>2025-01-15</td><td>4</td><td>13.795336317998563</td><td>A</td></tr><tr><td>2025-01-15</td><td>8</td><td>49.539320824317414</td><td>B</td></tr><tr><td>2025-01-15</td><td>9</td><td>60.99716667551187</td><td>B</td></tr><tr><td>2025-01-15</td><td>7</td><td>15.084687792490248</td><td>B</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "2025-01-15",
         3,
         73.44850394468924,
         "B"
        ],
        [
         "2025-01-15",
         4,
         13.795336317998563,
         "A"
        ],
        [
         "2025-01-15",
         8,
         49.539320824317414,
         "B"
        ],
        [
         "2025-01-15",
         9,
         60.99716667551187,
         "B"
        ],
        [
         "2025-01-15",
         7,
         15.084687792490248,
         "B"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "position_date",
         "type": "\"date\""
        },
        {
         "metadata": "{}",
         "name": "id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "value",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "category",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table Statistics:\n  • Calendar: 2,557 rows\n  • Source Data: 4,940 rows\n  • Destination: 0 rows (empty)\n  • Backfill Log: 0 rows (empty)\n"
     ]
    }
   ],
   "source": [
    "# Verify Setup - Quick Data Preview\n",
    "print(\"\\n\uD83D\uDCCA Quick Data Preview:\\n\")\n",
    "\n",
    "# Show sample from each table\n",
    "print(\"Calendar Table:\")\n",
    "display(spark.sql(f\"SELECT * FROM {CALENDAR_TABLE} WHERE calendar_date = '2025-01-15' LIMIT 5\"))\n",
    "\n",
    "print(\"Source Data Table:\")\n",
    "display(spark.sql(f\"SELECT * FROM {SOURCE_TABLE} WHERE position_date = '2025-01-15' LIMIT 5\"))\n",
    "\n",
    "print(\"Table Statistics:\")\n",
    "print(f\"  • Calendar: {spark.table(CALENDAR_TABLE).count():,} rows\")\n",
    "print(f\"  • Source Data: {spark.table(SOURCE_TABLE).count():,} rows\")\n",
    "print(f\"  • Destination: {spark.table(DEST_TABLE).count():,} rows (empty)\")\n",
    "print(f\"  • Backfill Log: {spark.table(BACKFILL_LOG_TABLE).count():,} rows (empty)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "99b430f3-5f01-445c-b3e0-4831c5b4def2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Setup Complete! \uD83C\uDF89\n",
    "\n",
    "All tables have been created and populated successfully.\n",
    "\n",
    "### What's Ready:\n",
    "- ✅ **Calendar Table** - Business day calendar (2020-2026)\n",
    "- ✅ **Source Table** - Sample data with business dates\n",
    "- ✅ **Destination Table** - Partitioned target (empty, ready for backfill)\n",
    "- ✅ **Backfill Log Table** - Operation tracking (empty, ready for use)\n",
    "\n",
    "### Next Steps:\n",
    "1. **`02_process_data.ipynb`** - Create the data processing notebook\n",
    "2. **`03_backfill_orchestrator.ipynb`** - Orchestrate and monitor backfill operations\n",
    "\n",
    "---\n",
    "*Note: This setup uses managed Delta tables for simplicity. All data is stored in your catalog's default location.*"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7316276366589520,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "01_setup",
   "widgets": {
    "catalog": {
     "currentValue": "demos",
     "nuid": "2340a723-e754-450c-86f0-dd6df5e4dde2",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "main",
      "label": "Catalog Name",
      "name": "catalog",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "main",
      "label": "Catalog Name",
      "name": "catalog",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "schema": {
     "currentValue": "backfill_demo",
     "nuid": "1ac82ca9-03fb-48a8-946f-4bdf7f3f4635",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "backfill_demo",
      "label": "Schema Name",
      "name": "schema",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "backfill_demo",
      "label": "Schema Name",
      "name": "schema",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}