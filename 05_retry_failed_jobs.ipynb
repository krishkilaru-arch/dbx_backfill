{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b622695-7467-43e4-b83f-9ceb09b8a0b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Backfill Demo: Retry Failed Jobs\n",
    "\n",
    "This notebook identifies and retries failed backfill jobs from the log table.\n",
    "\n",
    "## What It Does:\n",
    "1. Queries the backfill log table for failed jobs\n",
    "2. Optionally filters by date range or age\n",
    "3. Re-triggers the orchestrator for each failed position date\n",
    "4. Updates the log table with new run information\n",
    "\n",
    "## Usage:\n",
    "- **Schedule Daily:** Run this notebook every day to automatically retry yesterday's failures\n",
    "- **Manual Retry:** Adjust parameters to retry specific date ranges\n",
    "- **Batch Retry:** Can process multiple failures in one execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c582cfb2-3d85-48cc-9477-ea9272f2cd9d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\uD83D\uDCC1 Catalog: demos\n\uD83D\uDCC2 Schema: backfill_demo\n\uD83D\uDCCA Tables:\n  • Source: demos.backfill_demo.source_data\n  • Destination: demos.backfill_demo.destination_data\n  • Calendar: demos.backfill_demo.calendar\n  • Backfill Log: demos.backfill_demo.backfill_log\n\n\uD83D\uDCCB Log Table: demos.backfill_demo.backfill_log\n"
     ]
    }
   ],
   "source": [
    "# Setup and Configuration\n",
    "import sys\n",
    "from pyspark.sql import functions as F\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Get current notebook path dynamically\n",
    "notebook_path = dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get()\n",
    "workspace_path = f\"/Workspace{notebook_path}\"\n",
    "base_path = workspace_path.rsplit('/', 1)[0]\n",
    "\n",
    "# Add to sys.path for importing config\n",
    "sys.path.append(base_path)\n",
    "\n",
    "from config import BACKFILL_LOG_TABLE, print_config\n",
    "\n",
    "# Display configuration\n",
    "print_config()\n",
    "print(f\"\\n\uD83D\uDCCB Log Table: {BACKFILL_LOG_TABLE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49d7858f-b49d-44b0-933f-0d86a37be311",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 1: Configuration Setup\n",
    "\n",
    "Load the centralized configuration and display the log table being queried for failed jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75c02290-927f-4337-ab26-fa8c2ba6f483",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\uD83D\uDD27 Retry Configuration:\n  • Days Back: 1\n  • Date Range: Not specified to Not specified\n  • Max Retries: 10\n"
     ]
    }
   ],
   "source": [
    "# Parameters - Customize based on your retry strategy\n",
    "\n",
    "# Option 1: Retry failures from the last N days\n",
    "dbutils.widgets.text(\"retry_days_back\", \"1\", \"Days Back to Check (0=all)\")\n",
    "retry_days_back = int(dbutils.widgets.get(\"retry_days_back\"))\n",
    "\n",
    "# Option 2: Specific date range (leave empty to use retry_days_back)\n",
    "dbutils.widgets.text(\"start_date\", \"\", \"Start Date (YYYY-MM-DD, optional)\")\n",
    "dbutils.widgets.text(\"end_date\", \"\", \"End Date (YYYY-MM-DD, optional)\")\n",
    "\n",
    "start_date_str = dbutils.widgets.get(\"start_date\")\n",
    "end_date_str = dbutils.widgets.get(\"end_date\")\n",
    "\n",
    "# Option 3: Max number of retries to process in one run\n",
    "dbutils.widgets.text(\"max_retries\", \"10\", \"Max Jobs to Retry\")\n",
    "max_retries = int(dbutils.widgets.get(\"max_retries\"))\n",
    "\n",
    "print(f\"\uD83D\uDD27 Retry Configuration:\")\n",
    "print(f\"  • Days Back: {retry_days_back if retry_days_back > 0 else 'All'}\")\n",
    "print(f\"  • Date Range: {start_date_str or 'Not specified'} to {end_date_str or 'Not specified'}\")\n",
    "print(f\"  • Max Retries: {max_retries}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd1344a0-d109-4640-a216-109650897742",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 2: Configure Retry Parameters\n",
    "\n",
    "Customize the retry behavior using widgets:\n",
    "\n",
    "**retry_days_back**: Number of days to look back for failures (0 = all time)\n",
    "- Set to `1` for daily scheduled retries (yesterday's failures)\n",
    "- Set to `7` for weekly cleanup\n",
    "\n",
    "**start_date / end_date**: Specific date range (optional, overrides retry_days_back)\n",
    "- Use for targeted retries of specific periods\n",
    "\n",
    "**max_retries**: Maximum number of jobs to retry in one execution\n",
    "- Prevents overwhelming the system with too many simultaneous retries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03a3511a-b065-469e-9994-7ca536194bc0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\uD83D\uDCCA Table Schema Check:\n  • retry_metadata column exists: True\n  • Filtering: Only original failed runs (retry_metadata IS NULL)\n\n\uD83D\uDD0D Searching for failed jobs...\n\nQuery: \n        WITH failed_originals AS (\n            SELECT \n                position_date,\n                job_name,\n                job_id,\n                run_id as original_run_id,\n                status,\n                end_time,\n                error_message\n            FROM demos.backfill_demo.backfill_log\n            WHERE status = 'FAILED' AND retry_metadata IS NULL AND start_time >= '2025-11-27'\n        ),\n        retry_counts AS (\n            SELECT \n                position_date,\n                COUNT(*) - 1 as current_retry_count\n            FROM demos.backfill_demo.backfill_log\n            WHERE position_date IN (SELECT position_date FROM failed_originals)\n            GROUP BY position_date\n        )\n        SELECT \n            f.position_date,\n            f.job_name,\n            f.job_id,\n            f.original_run_id,\n            f.status,\n            f.end_time,\n            f.error_message,\n            COALESCE(r.current_retry_count, 0) as retry_count\n        FROM failed_originals f\n        LEFT JOIN retry_counts r ON f.position_date = r.position_date\n        ORDER BY f.position_date DESC\n        LIMIT 10\n    \n\n\uD83D\uDCCB Found 2 failed job(s) to retry:\n\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>position_date</th><th>job_name</th><th>job_id</th><th>original_run_id</th><th>status</th><th>end_time</th><th>error_message</th><th>retry_count</th></tr></thead><tbody><tr><td>2025-01-07</td><td>02_process_data</td><td>743256224103762</td><td>813573629431429</td><td>FAILED</td><td>2025-11-28T04:32:38.285Z</td><td>null</td><td>0</td></tr><tr><td>2025-01-06</td><td>02_process_data</td><td>743256224103762</td><td>215057038482639</td><td>FAILED</td><td>2025-11-28T04:32:53.618Z</td><td>null</td><td>0</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "2025-01-07",
         "02_process_data",
         "743256224103762",
         "813573629431429",
         "FAILED",
         "2025-11-28T04:32:38.285Z",
         null,
         0
        ],
        [
         "2025-01-06",
         "02_process_data",
         "743256224103762",
         "215057038482639",
         "FAILED",
         "2025-11-28T04:32:53.618Z",
         null,
         0
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "position_date",
         "type": "\"date\""
        },
        {
         "metadata": "{}",
         "name": "job_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "job_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "original_run_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "status",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "end_time",
         "type": "\"timestamp\""
        },
        {
         "metadata": "{}",
         "name": "error_message",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "retry_count",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Query Failed Jobs from Log Table\n",
    "\n",
    "# Check if retry_metadata column exists\n",
    "table_schema = spark.table(BACKFILL_LOG_TABLE).schema\n",
    "has_retry_metadata = \"retry_metadata\" in [field.name for field in table_schema.fields]\n",
    "\n",
    "print(f\"\uD83D\uDCCA Table Schema Check:\")\n",
    "print(f\"  • retry_metadata column exists: {has_retry_metadata}\")\n",
    "\n",
    "# Build the WHERE clause based on parameters\n",
    "where_clauses = [\"status = 'FAILED'\"]\n",
    "\n",
    "# Only filter by retry_metadata if the column exists\n",
    "if has_retry_metadata:\n",
    "    where_clauses.append(\"retry_metadata IS NULL\")  # Only get original failed runs, not failed retries\n",
    "    print(f\"  • Filtering: Only original failed runs (retry_metadata IS NULL)\")\n",
    "else:\n",
    "    print(f\"  • Warning: retry_metadata column not found. Run 03_backfill_orchestrator.ipynb cell 7 to add it.\")\n",
    "    print(f\"  • Filtering: All failed runs (cannot distinguish originals from retries)\")\n",
    "\n",
    "if start_date_str and end_date_str:\n",
    "    # Use specific date range\n",
    "    where_clauses.append(f\"start_time BETWEEN '{start_date_str}' AND '{end_date_str}'\")\n",
    "elif retry_days_back > 0:\n",
    "    # Calculate date threshold\n",
    "    cutoff_date = (datetime.now() - timedelta(days=retry_days_back)).strftime('%Y-%m-%d')\n",
    "    where_clauses.append(f\"start_time >= '{cutoff_date}'\")\n",
    "\n",
    "where_clause = \" AND \".join(where_clauses)\n",
    "\n",
    "# Query for failed jobs - get original runs with retry count\n",
    "if has_retry_metadata:\n",
    "    # Full query with retry tracking\n",
    "    query = f\"\"\"\n",
    "        WITH failed_originals AS (\n",
    "            SELECT \n",
    "                position_date,\n",
    "                job_name,\n",
    "                job_id,\n",
    "                run_id as original_run_id,\n",
    "                status,\n",
    "                end_time,\n",
    "                error_message\n",
    "            FROM {BACKFILL_LOG_TABLE}\n",
    "            WHERE {where_clause}\n",
    "        ),\n",
    "        retry_counts AS (\n",
    "            SELECT \n",
    "                position_date,\n",
    "                COUNT(*) - 1 as current_retry_count\n",
    "            FROM {BACKFILL_LOG_TABLE}\n",
    "            WHERE position_date IN (SELECT position_date FROM failed_originals)\n",
    "            GROUP BY position_date\n",
    "        )\n",
    "        SELECT \n",
    "            f.position_date,\n",
    "            f.job_name,\n",
    "            f.job_id,\n",
    "            f.original_run_id,\n",
    "            f.status,\n",
    "            f.end_time,\n",
    "            f.error_message,\n",
    "            COALESCE(r.current_retry_count, 0) as retry_count\n",
    "        FROM failed_originals f\n",
    "        LEFT JOIN retry_counts r ON f.position_date = r.position_date\n",
    "        ORDER BY f.position_date DESC\n",
    "        LIMIT {max_retries}\n",
    "    \"\"\"\n",
    "else:\n",
    "    # Simplified query without retry tracking\n",
    "    query = f\"\"\"\n",
    "        SELECT \n",
    "            position_date,\n",
    "            job_name,\n",
    "            job_id,\n",
    "            run_id as original_run_id,\n",
    "            status,\n",
    "            end_time,\n",
    "            error_message,\n",
    "            0 as retry_count\n",
    "        FROM {BACKFILL_LOG_TABLE}\n",
    "        WHERE {where_clause}\n",
    "        ORDER BY position_date DESC\n",
    "        LIMIT {max_retries}\n",
    "    \"\"\"\n",
    "\n",
    "print(f\"\\n\uD83D\uDD0D Searching for failed jobs...\\n\")\n",
    "print(f\"Query: {query}\\n\")\n",
    "\n",
    "failed_jobs_df = spark.sql(query)\n",
    "failed_jobs = failed_jobs_df.collect()\n",
    "\n",
    "if len(failed_jobs) == 0:\n",
    "    print(\"✅ No failed jobs found! Nothing to retry.\")\n",
    "    dbutils.notebook.exit(\"No failed jobs to retry\")\n",
    "else:\n",
    "    print(f\"\uD83D\uDCCB Found {len(failed_jobs)} failed job(s) to retry:\\n\")\n",
    "    display(failed_jobs_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ddec254-5f70-492e-88e2-847cb0f1f4ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 3: Query Failed Jobs\n",
    "\n",
    "Query the backfill log table for failed jobs matching the criteria:\n",
    "\n",
    "**Logic:**\n",
    "1. Check if `retry_metadata` column exists (backward compatible)\n",
    "2. Filter for `status = 'FAILED'` jobs\n",
    "3. If retry_metadata exists, only get original failures (not failed retries)\n",
    "4. Calculate retry count for each position_date\n",
    "5. Apply date filters based on parameters\n",
    "6. Limit results to max_retries\n",
    "\n",
    "**Retry Tracking:**\n",
    "- Original runs: `retry_metadata IS NULL`\n",
    "- Retry runs: `retry_metadata IS NOT NULL`\n",
    "- Retry count: Number of attempts for each position_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0bef995a-048e-40c2-8cc1-b327eba00d07",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\uD83C\uDFAF Using Orchestrator Job ID: 729227138192618\n\n\uD83D\uDD04 Starting retry process for 2 failed job(s)...\n\n================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Retry Logic - Trigger Orchestrator for Each Failed Date\n",
    "from databricks.sdk import WorkspaceClient\n",
    "import time\n",
    "\n",
    "w = WorkspaceClient()\n",
    "\n",
    "# Get the orchestrator job (assuming it's named \"03_backfill_orchestrator\")\n",
    "workspace_id = dbutils.entry_point.getDbutils().notebook().getContext().workspaceId().get()\n",
    "\n",
    "# Find the orchestrator job ID\n",
    "orchestrator_query = f\"\"\"\n",
    "    SELECT job_id, name\n",
    "    FROM system.lakeflow.jobs\n",
    "    WHERE workspace_id = '{workspace_id}'\n",
    "      AND name = '03_backfill_orchestrator'\n",
    "      AND delete_time IS NULL\n",
    "    ORDER BY change_time DESC\n",
    "    LIMIT 1\n",
    "\"\"\"\n",
    "\n",
    "orchestrator_df = spark.sql(orchestrator_query)\n",
    "if orchestrator_df.count() == 0:\n",
    "    print(\"❌ Orchestrator job '03_backfill_orchestrator' not found!\")\n",
    "    print(\"Please run 04_job_manager.ipynb first to create the orchestrator job.\")\n",
    "    dbutils.notebook.exit(\"Orchestrator job not found\")\n",
    "\n",
    "orchestrator_job_id = orchestrator_df.first()['job_id']\n",
    "print(f\"\uD83C\uDFAF Using Orchestrator Job ID: {orchestrator_job_id}\\n\")\n",
    "\n",
    "# Track retry results\n",
    "retry_results = []\n",
    "\n",
    "print(f\"\uD83D\uDD04 Starting retry process for {len(failed_jobs)} failed job(s)...\\n\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64698c8a-d98b-4c5a-97d5-879c0bbf27d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 4: Setup Orchestrator Connection\n",
    "\n",
    "Find the `03_backfill_orchestrator` job ID to trigger retries.\n",
    "\n",
    "**Note:** This requires that jobs have been created using `04_job_manager.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9db49f4d-85d2-411b-8241-84d0adef308a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n[1/2] Retrying: 2025-01-07\n  Original Job: 02_process_data (ID: 743256224103762)\n  Original Run ID: 813573629431429\n  Retry Attempt: #1\n  Original Error: None\n  ✓ Retry triggered successfully\n  ↳ New Run ID: 1023773776942719\n\n[2/2] Retrying: 2025-01-06\n  Original Job: 02_process_data (ID: 743256224103762)\n  Original Run ID: 215057038482639\n  Retry Attempt: #1\n  Original Error: None\n  ✓ Retry triggered successfully\n  ↳ New Run ID: 687943302127233\n\n================================================================================\n\n✅ Retry process complete!\n\n"
     ]
    }
   ],
   "source": [
    "# Execute Retries\n",
    "import json\n",
    "\n",
    "for idx, failed_job in enumerate(failed_jobs, 1):\n",
    "    position_date = str(failed_job['position_date'])\n",
    "    original_error = failed_job['error_message']\n",
    "    original_job_name = failed_job['job_name']\n",
    "    original_job_id = failed_job['job_id']\n",
    "    original_run_id = failed_job['original_run_id']\n",
    "    current_retry_count = failed_job['retry_count']\n",
    "    \n",
    "    # Build retry metadata JSON\n",
    "    retry_metadata = {\n",
    "        \"is_retry\": True,\n",
    "        \"original_run_id\": original_run_id,\n",
    "        \"retry_count\": current_retry_count + 1,\n",
    "        \"retry_triggered_by\": \"05_retry_notebook\",\n",
    "        \"retry_triggered_at\": datetime.now().isoformat()\n",
    "    }\n",
    "    retry_metadata_json = json.dumps(retry_metadata)\n",
    "    \n",
    "    print(f\"\\n[{idx}/{len(failed_jobs)}] Retrying: {position_date}\")\n",
    "    print(f\"  Original Job: {original_job_name} (ID: {original_job_id})\")\n",
    "    print(f\"  Original Run ID: {original_run_id}\")\n",
    "    print(f\"  Retry Attempt: #{current_retry_count + 1}\")\n",
    "    print(f\"  Original Error: {original_error}\")\n",
    "    \n",
    "    try:\n",
    "        # Trigger the orchestrator job with retry metadata\n",
    "        response = w.jobs.run_now(\n",
    "            job_id=orchestrator_job_id,\n",
    "            notebook_params={\n",
    "                \"position_date\": position_date,\n",
    "                \"job_name\": original_job_name,\n",
    "                \"retry_metadata\": retry_metadata_json\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        retry_run_id = response.run_id\n",
    "        print(f\"  ✓ Retry triggered successfully\")\n",
    "        print(f\"  ↳ New Run ID: {retry_run_id}\")\n",
    "        \n",
    "        # Store result\n",
    "        retry_results.append({\n",
    "            \"position_date\": position_date,\n",
    "            \"retry_status\": \"TRIGGERED\",\n",
    "            \"retry_run_id\": str(retry_run_id),\n",
    "            \"original_error\": original_error\n",
    "        })\n",
    "        \n",
    "        # Small delay to avoid overwhelming the API\n",
    "        time.sleep(2)\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = str(e)\n",
    "        print(f\"  ✗ Failed to trigger retry: {error_msg}\")\n",
    "        \n",
    "        retry_results.append({\n",
    "            \"position_date\": position_date,\n",
    "            \"retry_status\": \"TRIGGER_FAILED\",\n",
    "            \"retry_run_id\": None,\n",
    "            \"original_error\": original_error,\n",
    "            \"retry_error\": error_msg\n",
    "        })\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(f\"\\n✅ Retry process complete!\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "883b573b-1008-4183-be9d-fb4f6f9fda4a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 5: Execute Retries\n",
    "\n",
    "For each failed job, this cell:\n",
    "\n",
    "1. **Builds Retry Metadata JSON:**\n",
    "   ```json\n",
    "   {\n",
    "     \"is_retry\": true,\n",
    "     \"original_run_id\": \"<original_run_id>\",\n",
    "     \"retry_count\": <attempt_number>,\n",
    "     \"retry_triggered_by\": \"05_retry_notebook\",\n",
    "     \"retry_triggered_at\": \"<timestamp>\"\n",
    "   }\n",
    "   ```\n",
    "\n",
    "2. **Triggers Orchestrator Job:**\n",
    "   - Passes `position_date`, `job_name`, and `retry_metadata` as parameters\n",
    "   - Orchestrator validates business day and logs the retry\n",
    "\n",
    "3. **Tracks Results:**\n",
    "   - Stores triggered run IDs\n",
    "   - Captures any trigger failures\n",
    "   - Adds 2-second delay between triggers to avoid API rate limits\n",
    "\n",
    "**Retry Workflow:**\n",
    "```\n",
    "05_retry_failed_jobs.ipynb \n",
    "  → triggers → 03_backfill_orchestrator.ipynb (with retry metadata)\n",
    "    → triggers → 02_process_data.ipynb\n",
    "      → logs → backfill_log (with retry_metadata populated)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4cb48e48-e0fd-43f7-8300-95f71ea396aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\uD83D\uDCCA Retry Summary:\n\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>position_date</th><th>retry_status</th><th>retry_run_id</th><th>original_error</th></tr></thead><tbody><tr><td>2025-01-07</td><td>TRIGGERED</td><td>1023773776942719</td><td>null</td></tr><tr><td>2025-01-06</td><td>TRIGGERED</td><td>687943302127233</td><td>null</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "2025-01-07",
         "TRIGGERED",
         "1023773776942719",
         null
        ],
        [
         "2025-01-06",
         "TRIGGERED",
         "687943302127233",
         null
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "position_date",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "retry_status",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "retry_run_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "original_error",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n\uD83D\uDCC8 Statistics:\n  • Total Failed Jobs Found: 2\n  • Successfully Triggered: 2\n  • Failed to Trigger: 0\n\n\uD83D\uDCA1 Tip: Check the backfill log table to monitor the status of these retry runs.\n   Query: SELECT * FROM demos.backfill_demo.backfill_log WHERE backfill_job_id IN ('1023773776942719', '687943302127233')\n"
     ]
    }
   ],
   "source": [
    "# Summary Report\n",
    "\n",
    "# Convert results to DataFrame for easy viewing\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "retry_schema = StructType([\n",
    "    StructField(\"position_date\", StringType(), False),\n",
    "    StructField(\"retry_status\", StringType(), False),\n",
    "    StructField(\"retry_run_id\", StringType(), True),\n",
    "    StructField(\"original_error\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Create summary DataFrame\n",
    "summary_data = [{\n",
    "    \"position_date\": r[\"position_date\"],\n",
    "    \"retry_status\": r[\"retry_status\"],\n",
    "    \"retry_run_id\": r.get(\"retry_run_id\"),\n",
    "    \"original_error\": r.get(\"original_error\")\n",
    "} for r in retry_results]\n",
    "\n",
    "summary_df = spark.createDataFrame(summary_data, schema=retry_schema)\n",
    "\n",
    "# Display summary\n",
    "print(\"\uD83D\uDCCA Retry Summary:\\n\")\n",
    "display(summary_df)\n",
    "\n",
    "# Statistics\n",
    "triggered_count = len([r for r in retry_results if r[\"retry_status\"] == \"TRIGGERED\"])\n",
    "failed_count = len([r for r in retry_results if r[\"retry_status\"] == \"TRIGGER_FAILED\"])\n",
    "\n",
    "print(f\"\\n\uD83D\uDCC8 Statistics:\")\n",
    "print(f\"  • Total Failed Jobs Found: {len(failed_jobs)}\")\n",
    "print(f\"  • Successfully Triggered: {triggered_count}\")\n",
    "print(f\"  • Failed to Trigger: {failed_count}\")\n",
    "\n",
    "if triggered_count > 0:\n",
    "    # Build the run IDs list outside the f-string to avoid backslash in f-string\n",
    "    triggered_run_ids = ', '.join([f\"'{r['retry_run_id']}'\" for r in retry_results if r['retry_status'] == 'TRIGGERED'])\n",
    "    print(f\"\\n\uD83D\uDCA1 Tip: Check the backfill log table to monitor the status of these retry runs.\")\n",
    "    print(f\"   Query: SELECT * FROM {BACKFILL_LOG_TABLE} WHERE backfill_job_id IN ({triggered_run_ids})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1c1d7fe-cdee-4a70-aaaf-a849ffc16790",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 6: Display Retry Summary\n",
    "\n",
    "Show results of the retry process:\n",
    "- Position dates retried\n",
    "- Retry status (TRIGGERED or TRIGGER_FAILED)\n",
    "- New run IDs for monitoring\n",
    "- Original error messages for context\n",
    "\n",
    "**Statistics:**\n",
    "- Total failed jobs found\n",
    "- Successfully triggered retries\n",
    "- Failed to trigger (if any)\n",
    "\n",
    "**Next Steps:**\n",
    "- Monitor the backfill log table for retry outcomes\n",
    "- Check if retries succeeded or need further investigation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36738ab8-2311-4b62-a5f5-cc19e901ab89",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Retry Process Complete! ✅\n",
    "\n",
    "### What Happened:\n",
    "- ✅ Queried log table for failed jobs\n",
    "- ✅ Triggered orchestrator for each failed position date\n",
    "- ✅ New runs will be tracked in the log table with updated run IDs\n",
    "\n",
    "### Scheduling This Notebook:\n",
    "\n",
    "**Option 1: Daily Scheduled Job**\n",
    "```python\n",
    "# Run every day at 9 AM to retry yesterday's failures\n",
    "# Set parameters: retry_days_back = 1\n",
    "```\n",
    "\n",
    "**Option 2: Weekly Cleanup**\n",
    "```python\n",
    "# Run weekly to retry all failures from the past 7 days\n",
    "# Set parameters: retry_days_back = 7\n",
    "```\n",
    "\n",
    "**Option 3: Manual Investigation**\n",
    "```python\n",
    "# Run manually with specific date range\n",
    "# Set parameters: start_date, end_date\n",
    "```\n",
    "\n",
    "### Monitoring:\n",
    "- Check the backfill log table to see the status of retry runs\n",
    "- Each retry will have a new `backfill_job_id` in the log\n",
    "- Original failed runs remain in the log for audit purposes\n",
    "\n",
    "---\n",
    "*Tip: You can create a Databricks Job for this notebook using `04_job_manager.ipynb` to automate daily retries.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4118dfea-3488-4065-b333-66c96d7cabbb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## \uD83D\uDCCA Retry Tracking Queries\n",
    "\n",
    "Now that retry metadata is tracked, you can run powerful analytics:\n",
    "\n",
    "### **See all retry attempts for a position date:**\n",
    "```sql\n",
    "SELECT \n",
    "    run_id,\n",
    "    status,\n",
    "    start_time,\n",
    "    end_time,\n",
    "    retry_metadata:is_retry as is_retry,\n",
    "    retry_metadata:retry_count as retry_attempt,\n",
    "    retry_metadata:original_run_id as original_run_id,\n",
    "    error_message\n",
    "FROM demos.backfill_demo.backfill_log\n",
    "WHERE position_date = '2025-01-15'\n",
    "ORDER BY start_time;\n",
    "```\n",
    "\n",
    "### **Find jobs that succeeded after retries:**\n",
    "```sql\n",
    "SELECT \n",
    "    position_date,\n",
    "    COUNT(*) as total_attempts,\n",
    "    MAX(CAST(retry_metadata:retry_count AS INT)) as max_retry_count,\n",
    "    MIN(CASE WHEN status = 'FAILED' THEN start_time END) as first_failure,\n",
    "    MAX(CASE WHEN status = 'SUCCESS' THEN start_time END) as final_success\n",
    "FROM demos.backfill_demo.backfill_log\n",
    "WHERE retry_metadata IS NOT NULL\n",
    "GROUP BY position_date\n",
    "HAVING MAX(CASE WHEN status='SUCCESS' THEN 1 ELSE 0 END) = 1;\n",
    "```\n",
    "\n",
    "### **Retry success rate:**\n",
    "```sql\n",
    "SELECT \n",
    "    retry_metadata:retry_count as retry_attempt,\n",
    "    COUNT(*) as total_retries,\n",
    "    SUM(CASE WHEN status = 'SUCCESS' THEN 1 ELSE 0 END) as successful,\n",
    "    ROUND(100.0 * SUM(CASE WHEN status = 'SUCCESS' THEN 1 ELSE 0 END) / COUNT(*), 2) as success_rate_pct\n",
    "FROM demos.backfill_demo.backfill_log\n",
    "WHERE retry_metadata IS NOT NULL\n",
    "GROUP BY retry_metadata:retry_count\n",
    "ORDER BY retry_attempt;\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "05_retry_failed_jobs",
   "widgets": {
    "end_date": {
     "currentValue": "",
     "nuid": "164707d0-a41b-4077-98eb-1a79a7a14650",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "End Date (YYYY-MM-DD, optional)",
      "name": "end_date",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "End Date (YYYY-MM-DD, optional)",
      "name": "end_date",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    },
    "max_retries": {
     "currentValue": "10",
     "nuid": "46ebf195-37de-4a54-a295-029d7fc2b725",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "10",
      "label": "Max Jobs to Retry",
      "name": "max_retries",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "10",
      "label": "Max Jobs to Retry",
      "name": "max_retries",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    },
    "retry_days_back": {
     "currentValue": "1",
     "nuid": "90363595-d495-48ca-bf6e-4f288a3a33b3",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "1",
      "label": "Days Back to Check (0=all)",
      "name": "retry_days_back",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "1",
      "label": "Days Back to Check (0=all)",
      "name": "retry_days_back",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    },
    "start_date": {
     "currentValue": "",
     "nuid": "a90793c8-4711-4c5e-9e7a-d0ff2bf6eee7",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "Start Date (YYYY-MM-DD, optional)",
      "name": "start_date",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "Start Date (YYYY-MM-DD, optional)",
      "name": "start_date",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}