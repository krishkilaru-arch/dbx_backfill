{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9ef3abc8-8242-4957-a4b7-28dbcefc5273",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Backfill Demo: Process Data Notebook\n",
    "\n",
    "This notebook processes data for a single position date. It's designed to be run as a Databricks Job task.\n",
    "\n",
    "## What It Does:\n",
    "1. Accepts `position_date` parameter via widget\n",
    "2. Reads data from source table for that date\n",
    "3. Writes to destination table (partition overwrite for idempotency)\n",
    "\n",
    "**Note:** Business day validation is handled by the orchestrator, not in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94e52ecd-bed4-4cfc-a5f0-cf42e0773023",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\uD83D\uDCC5 Position Date: 2025-01-15\n? Source: demos.backfill_demo.source_data\n? Destination: demos.backfill_demo.destination_data\n"
     ]
    }
   ],
   "source": [
    "# Setup and Parameters\n",
    "import sys\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Get current notebook path dynamically\n",
    "notebook_path = dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get()\n",
    "workspace_path = f\"/Workspace{notebook_path}\"\n",
    "base_path = workspace_path.rsplit('/', 1)[0]\n",
    "\n",
    "# Add to sys.path for importing config\n",
    "sys.path.append(base_path)\n",
    "\n",
    "from config import SOURCE_TABLE, DEST_TABLE\n",
    "\n",
    "# Widget for position_date parameter\n",
    "dbutils.widgets.text(\"position_date\", \"\", \"Position Date (YYYY-MM-DD)\")\n",
    "position_date_str = dbutils.widgets.get(\"position_date\")\n",
    "\n",
    "print(f\"\uD83D\uDCC5 Position Date: {position_date_str}\")\n",
    "print(f\"? Source: {SOURCE_TABLE}\")\n",
    "print(f\"? Destination: {DEST_TABLE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "626a9dde-9c1d-4f01-a2ac-bcbd361e8ebc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n\uD83D\uDD04 Processing data for 2025-01-15...\n\uD83D\uDCCA Found 10 records\n✓ Successfully processed 2025-01-15\n  └─ 10 records written to demos.backfill_demo.destination_data\n"
     ]
    }
   ],
   "source": [
    "# Perform ETL - Read, Transform, Write\n",
    "try:\n",
    "    print(f\"\\n\uD83D\uDD04 Processing data for {position_date_str}...\")\n",
    "    \n",
    "    # Read source data for the specific date\n",
    "    # Cast position_date for proper comparison\n",
    "    df_source = (spark.table(SOURCE_TABLE)\n",
    "                 .filter(F.col(\"position_date\") == F.to_date(F.lit(position_date_str))))\n",
    "    \n",
    "    record_count = df_source.count()\n",
    "    \n",
    "    if record_count == 0:\n",
    "        print(f\"⚠️ No data found for {position_date_str}\")\n",
    "    else:\n",
    "        print(f\"\uD83D\uDCCA Found {record_count:,} records\")\n",
    "        \n",
    "        # Write to destination table using partition overwrite\n",
    "        # This ensures idempotency - reruns will replace data for this date\n",
    "        (df_source.write\n",
    "            .format(\"delta\")\n",
    "            .mode(\"overwrite\")\n",
    "            .option(\"replaceWhere\", f\"position_date = cast('{position_date_str}' as date)\")\n",
    "            .saveAsTable(DEST_TABLE))\n",
    "        \n",
    "        print(f\"✓ Successfully processed {position_date_str}\")\n",
    "        print(f\"  └─ {record_count:,} records written to {DEST_TABLE}\")\n",
    "\n",
    "except Exception as e:\n",
    "    error_msg = str(e)\n",
    "    print(f\"✗ Error processing {position_date_str}: {error_msg}\")\n",
    "    raise  # Re-raise to fail the job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "98b447db-78e5-422a-b92f-0772ac0e4eac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Notebook Complete! ✓\n",
    "\n",
    "This notebook is designed to be called by:\n",
    "- Databricks Workflow Jobs\n",
    "- Orchestration notebooks (like `03_backfill_orchestrator.ipynb`)\n",
    "- Manual runs for testing\n",
    "\n",
    "**Key Features:**\n",
    "- ✅ Simple ETL logic (read → write)\n",
    "- ✅ Idempotent writes (partition overwrite)\n",
    "- ✅ Error handling with proper exceptions\n",
    "- ✅ Clear logging and status messages\n",
    "\n",
    "**Note:** Business day validation should be handled by the caller/orchestrator."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02_process_data",
   "widgets": {
    "position_date": {
     "currentValue": "2025-01-15",
     "nuid": "81be545f-b69e-4dd9-98de-07aded42a189",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "Position Date (YYYY-MM-DD)",
      "name": "position_date",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "Position Date (YYYY-MM-DD)",
      "name": "position_date",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}