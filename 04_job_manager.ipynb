{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "76a1aca1-69ed-4245-8e0b-eb1b2ae53538",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Backfill Demo: Job Manager\n",
    "\n",
    "This notebook manages Databricks jobs for the backfill demo.\n",
    "\n",
    "## Functions:\n",
    "1. **List Jobs** - Show all existing jobs with the same name\n",
    "2. **Create/Update Jobs** - Create new jobs or update existing ones\n",
    "3. **Cleanup** - Delete old/duplicate jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3280ddd-7a32-4113-98dd-5befecc30359",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\uD83D\uDCC1 Base Path: /Workspace/Users/krish.kilaru@lumenalta.com/demos/back_fill\n\uD83D\uDD27 Workspace ID: 5584109198115548\n"
     ]
    }
   ],
   "source": [
    "# Setup\n",
    "import sys\n",
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks.sdk.service.jobs import JobSettings as Job\n",
    "\n",
    "# Get current notebook path dynamically\n",
    "notebook_path = dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get()\n",
    "workspace_path = f\"/Workspace{notebook_path}\"\n",
    "base_path = workspace_path.rsplit('/', 1)[0]\n",
    "\n",
    "# Initialize Databricks client\n",
    "w = WorkspaceClient()\n",
    "\n",
    "# Get workspace details\n",
    "workspace_id = dbutils.entry_point.getDbutils().notebook().getContext().workspaceId().get()\n",
    "\n",
    "print(f\"\uD83D\uDCC1 Base Path: {base_path}\")\n",
    "print(f\"\uD83D\uDD27 Workspace ID: {workspace_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "432ab3d7-4495-4453-99bd-f4e0b120671a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 1: Setup and Configuration\n",
    "\n",
    "Initialize the Databricks SDK WorkspaceClient and get workspace context for job management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5262859d-0614-432c-b7a3-55e09628ea76",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Helper functions loaded\n"
     ]
    }
   ],
   "source": [
    "# Function to list all jobs by name\n",
    "def list_jobs_by_name(job_name):\n",
    "    \"\"\"List all active jobs with the given name\"\"\"\n",
    "    query = f\"\"\"\n",
    "        SELECT DISTINCT job_id, name, creator_id, change_time\n",
    "        FROM system.lakeflow.jobs \n",
    "        WHERE workspace_id = '{workspace_id}' \n",
    "          AND name = '{job_name}'\n",
    "          AND delete_time IS NULL\n",
    "        ORDER BY change_time DESC\n",
    "    \"\"\"\n",
    "    df = spark.sql(query)\n",
    "    \n",
    "    # Verify jobs actually exist by checking with the API\n",
    "    active_jobs = []\n",
    "    for row in df.collect():\n",
    "        try:\n",
    "            w.jobs.get(job_id=row['job_id'])\n",
    "            active_jobs.append(row)\n",
    "        except:\n",
    "            # Job doesn't exist, skip it\n",
    "            pass\n",
    "    \n",
    "    # Return only verified active jobs\n",
    "    if active_jobs:\n",
    "        return spark.createDataFrame(active_jobs)\n",
    "    else:\n",
    "        # Return empty dataframe with same schema\n",
    "        return spark.createDataFrame([], df.schema)\n",
    "    \n",
    "def list_jobs_by_name_raw(job_name):\n",
    "    \"\"\"List all jobs from metadata (may include deleted/stale jobs)\"\"\"\n",
    "    query = f\"\"\"\n",
    "        SELECT job_id, name, creator_id, change_time, delete_time\n",
    "        FROM system.lakeflow.jobs \n",
    "        WHERE workspace_id = '{workspace_id}' \n",
    "          AND name = '{job_name}'\n",
    "        ORDER BY change_time DESC\n",
    "    \"\"\"\n",
    "    return spark.sql(query)\n",
    "\n",
    "# Function to delete all jobs with a given name except the latest\n",
    "def cleanup_duplicate_jobs(job_name, keep_latest=True):\n",
    "    \"\"\"Delete duplicate jobs, optionally keeping the latest one\"\"\"\n",
    "    df = list_jobs_by_name(job_name)\n",
    "    jobs = df.collect()\n",
    "    \n",
    "    if len(jobs) == 0:\n",
    "        print(f\"No jobs found with name: {job_name}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found {len(jobs)} job(s) with name '{job_name}':\")\n",
    "    for job in jobs:\n",
    "        print(f\"  • Job ID: {job['job_id']} | Created: {job['change_time']}\")\n",
    "    \n",
    "    if keep_latest and len(jobs) > 1:\n",
    "        jobs_to_delete = jobs[1:]  # Skip the first (latest) one\n",
    "        print(f\"\\n\uD83D\uDDD1️ Deleting {len(jobs_to_delete)} old job(s)...\")\n",
    "        \n",
    "        for job in jobs_to_delete:\n",
    "            try:\n",
    "                w.jobs.delete(job_id=job['job_id'])\n",
    "                print(f\"  ✓ Deleted job ID: {job['job_id']}\")\n",
    "            except Exception as e:\n",
    "                print(f\"  ✗ Failed to delete job ID {job['job_id']}: {e}\")\n",
    "    elif not keep_latest:\n",
    "        print(f\"\\n\uD83D\uDDD1️ Deleting all {len(jobs)} job(s)...\")\n",
    "        for job in jobs:\n",
    "            try:\n",
    "                w.jobs.delete(job_id=job['job_id'])\n",
    "                print(f\"  ✓ Deleted job ID: {job['job_id']}\")\n",
    "            except Exception as e:\n",
    "                print(f\"  ✗ Failed to delete job ID {job['job_id']}: {e}\")\n",
    "    else:\n",
    "        print(f\"\\n✓ Only 1 job found, no cleanup needed\")\n",
    "\n",
    "# Function to create or update job\n",
    "def create_or_update_job(job_name, job_config, force_create=False):\n",
    "    \"\"\"Create a new job or update existing one\"\"\"\n",
    "    df = list_jobs_by_name(job_name)\n",
    "    jobs = df.collect()\n",
    "    \n",
    "    if len(jobs) > 0 and not force_create:\n",
    "        latest_job = jobs[0]\n",
    "        job_id = latest_job['job_id']\n",
    "        print(f\"\uD83D\uDCDD Attempting to update existing job: {job_name} (ID: {job_id})\")\n",
    "        \n",
    "        try:\n",
    "            # Verify job still exists by trying to get it\n",
    "            w.jobs.get(job_id=job_id)\n",
    "            w.jobs.reset(job_id=job_id, new_settings=job_config)\n",
    "            print(f\"✓ Job updated successfully\")\n",
    "            return job_id\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Job {job_id} not found or inaccessible: {e}\")\n",
    "            print(f\"➕ Creating new job instead...\")\n",
    "            # Build kwargs dynamically to only include attributes that exist\n",
    "            create_kwargs = {\n",
    "                \"name\": job_config.name,\n",
    "                \"tasks\": job_config.tasks,\n",
    "            }\n",
    "            if hasattr(job_config, 'max_concurrent_runs') and job_config.max_concurrent_runs:\n",
    "                create_kwargs[\"max_concurrent_runs\"] = job_config.max_concurrent_runs\n",
    "            if hasattr(job_config, 'tags') and job_config.tags:\n",
    "                create_kwargs[\"tags\"] = job_config.tags\n",
    "            if hasattr(job_config, 'queue') and job_config.queue:\n",
    "                create_kwargs[\"queue\"] = job_config.queue\n",
    "            \n",
    "            response = w.jobs.create(**create_kwargs)\n",
    "            print(f\"✓ Job created with ID: {response.job_id}\")\n",
    "            return response.job_id\n",
    "    else:\n",
    "        print(f\"➕ Creating new job: {job_name}\")\n",
    "        # Build kwargs dynamically to only include attributes that exist\n",
    "        create_kwargs = {\n",
    "            \"name\": job_config.name,\n",
    "            \"tasks\": job_config.tasks,\n",
    "        }\n",
    "        if hasattr(job_config, 'max_concurrent_runs') and job_config.max_concurrent_runs:\n",
    "            create_kwargs[\"max_concurrent_runs\"] = job_config.max_concurrent_runs\n",
    "        if hasattr(job_config, 'tags') and job_config.tags:\n",
    "            create_kwargs[\"tags\"] = job_config.tags\n",
    "        if hasattr(job_config, 'queue') and job_config.queue:\n",
    "            create_kwargs[\"queue\"] = job_config.queue\n",
    "        \n",
    "        response = w.jobs.create(**create_kwargs)\n",
    "        print(f\"✓ Job created with ID: {response.job_id}\")\n",
    "        return response.job_id\n",
    "\n",
    "print(\"✓ Helper functions loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4d1febea-4b7d-4dc2-90a4-5bbbf58e49ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 2: Helper Functions\n",
    "\n",
    "These utility functions help manage Databricks Jobs:\n",
    "\n",
    "1. **list_jobs_by_name()**: Query and verify active jobs (handles stale metadata)\n",
    "2. **list_jobs_by_name_raw()**: View all job metadata including deleted jobs\n",
    "3. **cleanup_duplicate_jobs()**: Remove duplicate job definitions\n",
    "4. **create_or_update_job()**: Smart function that updates existing jobs or creates new ones\n",
    "\n",
    "**Note:** The helper functions include dual verification (metadata + API) to handle stale job references."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7efdcbba-06a3-46d4-8806-33cd85892d85",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 1: Cleanup Duplicate Jobs\n",
    "\n",
    "Run this cell to remove duplicate jobs and keep only the latest version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0844f0a2-0d33-48e0-a88f-20de2bd808bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\uD83E\uDDF9 Cleaning up '02_process_data' jobs...\nNo jobs found with name: 02_process_data\n\n============================================================\n\n\uD83E\uDDF9 Cleaning up '03_backfill_orchestrator' jobs...\nNo jobs found with name: 03_backfill_orchestrator\n"
     ]
    }
   ],
   "source": [
    "# Cleanup duplicates for process_data job\n",
    "print(\"\uD83E\uDDF9 Cleaning up '02_process_data' jobs...\")\n",
    "cleanup_duplicate_jobs(\"02_process_data\", keep_latest=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# Cleanup duplicates for orchestrator job\n",
    "print(\"\uD83E\uDDF9 Cleaning up '03_backfill_orchestrator' jobs...\")\n",
    "cleanup_duplicate_jobs(\"03_backfill_orchestrator\", keep_latest=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "786a5a5a-55cd-465a-afa6-e8ff204db118",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 3: Cleanup Duplicate Jobs (Optional)\n",
    "\n",
    "Run this cell to remove duplicate job definitions and keep only the latest version.\n",
    "\n",
    "**When to use:**\n",
    "- After repeatedly testing job creation during development\n",
    "- To clean up test jobs\n",
    "- To maintain a single active job version\n",
    "\n",
    "**Safe to run:** This cell will keep the latest job version and only delete duplicates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fdbd8f8b-d43c-4fb9-9200-094edbcf9a74",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 2: Create/Update Jobs\n",
    "\n",
    "Define and create or update the jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4af35b55-8f79-487f-9412-e8fd2ab4f40a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "➕ Creating new job: 02_process_data\n✓ Job created with ID: 743256224103762\n\n✓ Process Data Job ID: 743256224103762\n"
     ]
    }
   ],
   "source": [
    "# Job 1: Process Data Job\n",
    "process_data_config = Job.from_dict({\n",
    "    \"name\": \"02_process_data\",\n",
    "    \"max_concurrent_runs\": 20,\n",
    "    \"tasks\": [{\n",
    "        \"task_key\": \"process_data\",\n",
    "        \"notebook_task\": {\n",
    "            \"notebook_path\": f\"{base_path}/02_process_data\",\n",
    "            \"base_parameters\": {\n",
    "                \"position_date\": \"\",\n",
    "            },\n",
    "            \"source\": \"WORKSPACE\",\n",
    "        },\n",
    "        \"max_retries\": 3,\n",
    "        \"min_retry_interval_millis\": 60000,\n",
    "        \"retry_on_timeout\": True,\n",
    "        \"timeout_seconds\": 1800,  # 30 minutes\n",
    "    }],\n",
    "    \"tags\": {\n",
    "        \"app_name\": \"backfill_demo\",\n",
    "        \"environment\": \"dev\"\n",
    "    },\n",
    "    \"queue\": {\n",
    "        \"enabled\": True,\n",
    "    },\n",
    "    \"performance_target\": \"PERFORMANCE_OPTIMIZED\",\n",
    "})\n",
    "\n",
    "# Create or update the job\n",
    "process_job_id = create_or_update_job(\"02_process_data\", process_data_config)\n",
    "print(f\"\\n✓ Process Data Job ID: {process_job_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ac8bed2e-87df-433e-a6e0-42bc504ba4ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 4: Define Job Configurations\n",
    "\n",
    "Configure the two main jobs for the backfill system:\n",
    "\n",
    "### Job 1: Process Data Job (`02_process_data`)\n",
    "- **Purpose**: Worker job that processes data for a single position_date\n",
    "- **Max Concurrent Runs**: 20 (process up to 20 dates in parallel)\n",
    "- **Retry Settings**: Up to 3 retries with 1-minute intervals\n",
    "- **Timeout**: 30 minutes per date\n",
    "- **Parameters**: Accepts `position_date` (YYYY-MM-DD)\n",
    "\n",
    "### Job 2: Orchestrator Job (`03_backfill_orchestrator`)\n",
    "- **Purpose**: Orchestrates backfill operations with business day validation and logging\n",
    "- **Max Concurrent Runs**: 10 (up to 10 parallel orchestrations)\n",
    "- **Retry Settings**: Up to 2 retries with 2-minute intervals\n",
    "- **Timeout**: 1 hour per orchestration\n",
    "- **Parameters**: Accepts `position_date`, `job_name`, and optional `retry_metadata`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56547fe4-7873-49d0-9762-42a132773f08",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "➕ Creating new job: 03_backfill_orchestrator\n✓ Job created with ID: 729227138192618\n\n✓ Backfill Orchestrator Job ID: 729227138192618\n"
     ]
    }
   ],
   "source": [
    "# Job 2: Backfill Orchestrator Job\n",
    "orchestrator_config = Job.from_dict({\n",
    "    \"name\": \"03_backfill_orchestrator\",\n",
    "    \"max_concurrent_runs\": 10,\n",
    "    \"tasks\": [{\n",
    "        \"task_key\": \"orchestrate_backfill\",\n",
    "        \"notebook_task\": {\n",
    "            \"notebook_path\": f\"{base_path}/03_backfill_orchestrator\",\n",
    "            \"base_parameters\": {\n",
    "                \"start_date\": \"\",\n",
    "                \"end_date\": \"\",\n",
    "                \"job_name\": \"02_process_data\",\n",
    "            },\n",
    "            \"source\": \"WORKSPACE\",\n",
    "        },\n",
    "        \"max_retries\": 2,\n",
    "        \"min_retry_interval_millis\": 120000,\n",
    "        \"retry_on_timeout\": False,\n",
    "        \"timeout_seconds\": 3600,  # 1 hour\n",
    "    }],\n",
    "    \"tags\": {\n",
    "        \"app_name\": \"backfill_demo\",\n",
    "        \"environment\": \"dev\"\n",
    "    },\n",
    "    \"queue\": {\n",
    "        \"enabled\": True,\n",
    "    },\n",
    "    \"performance_target\": \"PERFORMANCE_OPTIMIZED\",\n",
    "})\n",
    "\n",
    "# Create or update the job\n",
    "orchestrator_job_id = create_or_update_job(\"03_backfill_orchestrator\", orchestrator_config)\n",
    "print(f\"\\n✓ Backfill Orchestrator Job ID: {orchestrator_job_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "23c92b06-3e1f-47a9-8ce4-bc6dfcabb5f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 5: Create/Update Process Data Job\n",
    "\n",
    "Create or update the `02_process_data` job with the defined configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6f53aa5f-6f0e-4990-8f09-5be3ddafd116",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 3: Verify Jobs\n",
    "\n",
    "List all current jobs to verify the setup."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0b965eb9-9421-4f31-ad7c-a733a1a40bf9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 6: Create/Update Orchestrator Job\n",
    "\n",
    "Create or update the `03_backfill_orchestrator` job with the defined configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7026f39f-62ca-4635-bfdb-2493d679636a",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"name\":124},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1763704065199}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\uD83D\uDCCB Final Job Status:\n\n\n⏳ Waiting for jobs to appear... Retrying in 60 seconds.\n\n\n⏳ Waiting for jobs to appear... Retrying in 60 seconds.\n\n1. Process Data Jobs:\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>job_id</th><th>name</th><th>creator_id</th><th>change_time</th></tr></thead><tbody><tr><td>743256224103762</td><td>02_process_data</td><td>75053393473744</td><td>2025-11-21T05:39:37.396Z</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "743256224103762",
         "02_process_data",
         "75053393473744",
         "2025-11-21T05:39:37.396Z"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "job_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "creator_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "change_time",
         "type": "\"timestamp\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n2. Backfill Orchestrator Jobs:\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>job_id</th><th>name</th><th>creator_id</th><th>change_time</th></tr></thead><tbody><tr><td>729227138192618</td><td>03_backfill_orchestrator</td><td>75053393473744</td><td>2025-11-21T05:39:50.099Z</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "729227138192618",
         "03_backfill_orchestrator",
         "75053393473744",
         "2025-11-21T05:39:50.099Z"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "job_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "creator_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "change_time",
         "type": "\"timestamp\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Verify final state\n",
    "import time\n",
    "\n",
    "print(\"\uD83D\uDCCB Final Job Status:\\n\")\n",
    "\n",
    "while True:\n",
    "    process_jobs_df = list_jobs_by_name(\"02_process_data\")\n",
    "    orchestrator_jobs_df = list_jobs_by_name(\"03_backfill_orchestrator\")\n",
    "    \n",
    "    process_count = process_jobs_df.count()\n",
    "    orchestrator_count = orchestrator_jobs_df.count()  \n",
    "    \n",
    "    if process_count > 0 and orchestrator_count > 0:\n",
    "        print(\"1. Process Data Jobs:\")\n",
    "        display(process_jobs_df)\n",
    "        \n",
    "        print(\"\\n2. Backfill Orchestrator Jobs:\")\n",
    "        display(orchestrator_jobs_df)\n",
    "\n",
    "        break\n",
    "\n",
    "    print(\"\\n⏳ Waiting for jobs to appear... Retrying in 60 seconds.\\n\")\n",
    "    time.sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b005ad61-9a75-4d46-a478-0a0dee91fe71",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 7: Verify Job Creation\n",
    "\n",
    "Query and display all created jobs to verify successful setup.\n",
    "\n",
    "**Note:** There may be a brief delay before jobs appear in the system. This cell will retry automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e2dca03f-4c29-4a07-a98e-9a0f575c3589",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Job Management Complete! ✓\n",
    "\n",
    "**Available Jobs:**\n",
    "- ✅ **02_process_data** - Processes data for a single position date\n",
    "- ✅ **03_backfill_orchestrator** - Orchestrates backfill across multiple dates\n",
    "\n",
    "**Usage:**\n",
    "- Update job definitions in cells 6-7 and rerun to apply changes\n",
    "- Use Step 1 to cleanup duplicates anytime\n",
    "- Job IDs are displayed after creation/update"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "04_job_manager",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}