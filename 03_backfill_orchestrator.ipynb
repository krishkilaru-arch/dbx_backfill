{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1d02398d-edd1-4ae4-8708-0d83048a3234",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Backfill Demo: Orchestrator Notebook\n",
    "\n",
    "This notebook orchestrates backfill operations with business day validation and comprehensive logging.\n",
    "\n",
    "## What It Does:\n",
    "1. Accepts `position_date` and `job_name` parameters via widgets\n",
    "2. Validates if the date is a business day using the calendar table\n",
    "3. Skips non-business days (weekends/holidays) with logging\n",
    "4. Triggers the processing job for valid business dates\n",
    "5. Monitors job execution until completion\n",
    "6. Logs all operations (START/SUCCESS/FAILED/SKIPPED) to the backfill log table\n",
    "7. Supports retry tracking with JSON metadata for failed job retries\n",
    "\n",
    "## Key Features:\n",
    "- **Business Day Validation**: Queries calendar table to avoid processing holidays/weekends\n",
    "- **Job Triggering**: Uses Databricks SDK to trigger and monitor jobs programmatically\n",
    "- **Comprehensive Logging**: Tracks all operations with timestamps, status, and error details\n",
    "- **Retry Support**: Accepts retry metadata from the retry notebook to track retry attempts\n",
    "- **Backward Compatible**: Works with or without the retry_metadata column in the log table\n",
    "\n",
    "## Usage:\n",
    "- **Single Date**: Set `position_date` widget and run notebook\n",
    "- **Date Range**: Call this notebook in a loop for multiple dates (use Databricks SDK)\n",
    "- **Retry Failed Jobs**: Called by `05_retry_failed_jobs.ipynb` with retry metadata\n",
    "\n",
    "**Note:** This notebook is designed to be called programmatically or run interactively for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8eab7208-6da7-4e52-8738-b5865b8b9740",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Setup and Parameters\n",
    "import sys\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Get current notebook path dynamically\n",
    "notebook_path = dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get()\n",
    "workspace_path = f\"/Workspace{notebook_path}\"\n",
    "base_path = workspace_path.rsplit('/', 1)[0]\n",
    "\n",
    "# Add to sys.path for importing config\n",
    "sys.path.append(base_path)\n",
    "\n",
    "from config import BACKFILL_LOG_TABLE, CALENDAR_TABLE\n",
    "log_table = BACKFILL_LOG_TABLE\n",
    "calendar_table = CALENDAR_TABLE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fc36b972-78e3-4c85-815e-1a11d6cec16b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 1: Configuration Setup\n",
    "\n",
    "Load the centralized configuration from `config.py` to get table names and connection details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2237f9c0-b685-4c87-8287-0f0e6fe2478f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Retrieve and Display Current Workspace Identifier"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "'5584109198115548'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "workspace_id = dbutils.entry_point.getDbutils().notebook().getContext().workspaceId().get()\n",
    "display(workspace_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3c8fbe90-9c39-4450-9310-19cecc27af17",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 2: Get Workspace Context\n",
    "\n",
    "Retrieve the workspace ID for querying job metadata from the system.lakeflow.jobs table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f67ed57-c296-464c-9dec-f0d886a750c6",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Get and Show Position Date and Job Name Inputs"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "{'position_date': '2025-01-03',\n",
       " 'job_name': '02_process_data',\n",
       " 'retry_metadata': None}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dbutils.widgets.text(\"position_date\", \"\", \"Position Date (YYYY-MM-DD)\")\n",
    "position_date_str = dbutils.widgets.get(\"position_date\")\n",
    "\n",
    "dbutils.widgets.text(\"job_name\", \"\", \"Job Name\")\n",
    "job_name = dbutils.widgets.get(\"job_name\")\n",
    "\n",
    "# Retry metadata as JSON string (NULL for original runs)\n",
    "dbutils.widgets.text(\"retry_metadata\", \"\", \"Retry Metadata (JSON)\")\n",
    "retry_metadata_raw = dbutils.widgets.get(\"retry_metadata\")\n",
    "# Only use retry_metadata if it's a non-empty string\n",
    "retry_metadata_json = retry_metadata_raw if retry_metadata_raw and retry_metadata_raw.strip() else None\n",
    "\n",
    "display({\"position_date\": position_date_str, \"job_name\": job_name, \"retry_metadata\": retry_metadata_json})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "347eb8f4-e653-4a3a-bf62-2e9cbe9c38c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 3: Widget Parameters\n",
    "\n",
    "Accept input parameters for the backfill operation:\n",
    "- **position_date**: The date to process (YYYY-MM-DD format)\n",
    "- **job_name**: The name of the job to trigger (e.g., \"02_process_data\")\n",
    "- **retry_metadata**: JSON metadata for retry tracking (optional, used by retry notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76a153d3-ad50-492c-a368-61d84aaf5fb9",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Extract Distinct Job ID for Specified Job Name"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>account_id</th><th>workspace_id</th><th>job_id</th><th>name</th><th>creator_id</th><th>tags</th><th>run_as</th><th>change_time</th><th>delete_time</th><th>description</th></tr></thead><tbody><tr><td>31ee97a6-24bd-4274-9ac4-00fbbbb94321</td><td>5584109198115548</td><td>743256224103762</td><td>02_process_data</td><td>75053393473744</td><td>Map(app_name -> backfill_demo, environment -> dev)</td><td>75053393473744</td><td>2025-11-21T05:39:37.396Z</td><td>null</td><td>null</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "31ee97a6-24bd-4274-9ac4-00fbbbb94321",
         "5584109198115548",
         "743256224103762",
         "02_process_data",
         "75053393473744",
         {
          "app_name": "backfill_demo",
          "environment": "dev"
         },
         "75053393473744",
         "2025-11-21T05:39:37.396Z",
         null,
         null
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{\"comment\": \"The ID of the account this job belongs to.\"}",
         "name": "account_id",
         "type": "\"string\""
        },
        {
         "metadata": "{\"comment\": \"The ID of the workspace this job belongs to.\"}",
         "name": "workspace_id",
         "type": "\"string\""
        },
        {
         "metadata": "{\"comment\": \"The ID of the job. Only unique within a single workspace.\"}",
         "name": "job_id",
         "type": "\"string\""
        },
        {
         "metadata": "{\"comment\": \"The user-supplied name of the job.\"}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{\"comment\": \"The ID of the principal who created the job.\"}",
         "name": "creator_id",
         "type": "\"string\""
        },
        {
         "metadata": "{\"comment\": \"The user-supplied custom tags associated with this job.\"}",
         "name": "tags",
         "type": "{\"keyType\":\"string\",\"type\":\"map\",\"valueContainsNull\":true,\"valueType\":\"string\"}"
        },
        {
         "metadata": "{\"comment\": \"The ID of the user or service principal whose permissions are used for the job run.\"}",
         "name": "run_as",
         "type": "\"string\""
        },
        {
         "metadata": "{\"comment\": \"The time when the job was last modified. Timezone recorded as +00:00 (UTC).\"}",
         "name": "change_time",
         "type": "\"timestamp\""
        },
        {
         "metadata": "{\"comment\": \"The time when the job was deleted by the user. Timezone recorded as +00:00 (UTC).\"}",
         "name": "delete_time",
         "type": "\"timestamp\""
        },
        {
         "metadata": "{\"comment\": \"The user-supplied description of the job. **Not populated for rows emitted before late August 2024.**\"}",
         "name": "description",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'743256224103762'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = (spark.table(\"system.lakeflow.jobs\")\n",
    "        .filter(f\"workspace_id = '{workspace_id}' and name = '{job_name}'\")\n",
    "        .orderBy(\"change_time\", ascending=False)\n",
    "        .limit(1))\n",
    "display(df)\n",
    "\n",
    "job_id = df.select(\"job_id\").first()[\"job_id\"]\n",
    "job_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2abfec25-bbb0-4a49-87c6-d87856662fb2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 4: Lookup Job ID\n",
    "\n",
    "Query the `system.lakeflow.jobs` table to find the job ID for the specified job name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42225dda-38fa-48c4-afe1-009975e4efcb",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Determine and Print Current Job or Notebook Run ID"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downstream Job ID: 774114517055437\nRunning Mode: Interactive Cluster\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Get the downstream job ID (current workflow/job ID)\n",
    "context = dbutils.notebook.entry_point.getDbutils().notebook().getContext()\n",
    "if context.jobId().isDefined():\n",
    "    # Running as part of a job/workflow\n",
    "    job_run_id = context.jobRunId().get()\n",
    "    print(f\"Downstream Job Run ID: {job_run_id}\")\n",
    "    print(f\"Running Mode: Job/Workflow\")\n",
    "else:\n",
    "    # Running on interactive cluster\n",
    "    notebook_id = context.notebookId().get() if context.notebookId().isDefined() else \"unknown_notebook\"\n",
    "    job_run_id = f\"{notebook_id}\"\n",
    "    print(f\"Downstream Job ID: {job_run_id}\")\n",
    "    print(f\"Running Mode: Interactive Cluster\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "73470917-3c83-40fa-80ec-a0625f4c611a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 5: Get Orchestrator Context\n",
    "\n",
    "Determine the run ID for this orchestration job (for logging purposes):\n",
    "- If running as a Databricks Job/Workflow: Use jobRunId\n",
    "- If running on interactive cluster: Use notebookId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d45e0219-b0a3-4caa-a532-8e77233332c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ BackfillLogger class defined\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, DateType, TimestampType\n",
    "from datetime import datetime\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "\n",
    "\n",
    "# Define schema once\n",
    "log_schema = StructType([\n",
    "    StructField(\"run_id\", StringType(), False),\n",
    "    StructField(\"position_date\", DateType(), True),\n",
    "    StructField(\"job_name\", StringType(), True),\n",
    "    StructField(\"job_id\", StringType(), True),\n",
    "    StructField(\"status\", StringType(), True),\n",
    "    StructField(\"start_time\", TimestampType(), True),\n",
    "    StructField(\"end_time\", TimestampType(), True),\n",
    "    StructField(\"backfill_job_id\", StringType(), True),\n",
    "    StructField(\"error_message\", StringType(), True),\n",
    "    StructField(\"creator_user_name\", StringType(), True),\n",
    "    StructField(\"run_page_url\", StringType(), True),\n",
    "    StructField(\"retry_metadata\", StringType(), True)\n",
    "])\n",
    "\n",
    "class BackfillLogger:\n",
    "    \"\"\"\n",
    "    Simple, intuitive logger for backfill operations.\n",
    "    Usage:\n",
    "        logger = BackfillLogger(run_id, position_date, job_name, job_id, retry_metadata_json=None)\n",
    "        logger.start()\n",
    "        logger.success()  # or logger.fail(\"error message\")\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, run_id, position_date, job_name, job_id, retry_metadata_json=None):\n",
    "        self.run_id = str(run_id)  # Ensure string\n",
    "        self.position_date = position_date\n",
    "        self.job_name = str(job_name)  # Ensure string\n",
    "        self.job_id = str(job_id)  # Ensure string\n",
    "        self.start_time = datetime.now()\n",
    "        self.retry_metadata_json = retry_metadata_json  # JSON string or None\n",
    "        \n",
    "    def _upsert(self, status, backfill_job_id=None, error_message=None, creator_user_name=None, run_page_url=None):\n",
    "        \"\"\"Internal method to upsert log record.\"\"\"\n",
    "        # Parse position_date if string\n",
    "        if isinstance(self.position_date, str):\n",
    "            pos_date = datetime.strptime(self.position_date, \"%Y-%m-%d\").date()\n",
    "        else:\n",
    "            pos_date = self.position_date\n",
    "        \n",
    "        # Auto-set end_time for terminal states\n",
    "        end_time = datetime.now() if status in ['SUCCESS', 'FAILED', 'SKIPPED'] else None\n",
    "        \n",
    "        # Check which columns exist in the target table\n",
    "        delta_table = DeltaTable.forName(spark, log_table)\n",
    "        existing_columns = set([field.name for field in delta_table.toDF().schema.fields])\n",
    "        \n",
    "        # Create record with explicit types - ensure no None for required fields\n",
    "        record = [{\n",
    "            \"run_id\": self.run_id,\n",
    "            \"position_date\": pos_date,\n",
    "            \"job_name\": self.job_name,\n",
    "            \"job_id\": self.job_id,\n",
    "            \"status\": status,\n",
    "            \"start_time\": self.start_time,\n",
    "            \"end_time\": end_time,\n",
    "            \"backfill_job_id\": backfill_job_id if backfill_job_id else \"\",\n",
    "            \"error_message\": error_message if error_message else None,\n",
    "            \"creator_user_name\": creator_user_name if creator_user_name else None,\n",
    "            \"run_page_url\": run_page_url if run_page_url else None,\n",
    "            \"retry_metadata\": self.retry_metadata_json\n",
    "        }]\n",
    "        \n",
    "        # Create DataFrame with explicit schema\n",
    "        source_df = spark.createDataFrame(record, schema=log_schema)\n",
    "        \n",
    "        # Debug: Show what we're trying to insert\n",
    "        print(f\"DEBUG: Upserting - run_id={self.run_id}, pos_date={pos_date}, job_name={self.job_name}, job_id={self.job_id}, status={status}\")\n",
    "        \n",
    "        # Build update/insert dictionaries dynamically based on existing columns\n",
    "        update_set = {\n",
    "            \"status\": \"s.status\",\n",
    "            \"end_time\": \"s.end_time\",\n",
    "            \"backfill_job_id\": \"s.backfill_job_id\",\n",
    "            \"error_message\": \"s.error_message\"\n",
    "        }\n",
    "        \n",
    "        insert_values = {\n",
    "            \"run_id\": \"s.run_id\",\n",
    "            \"position_date\": \"s.position_date\",\n",
    "            \"job_name\": \"s.job_name\",\n",
    "            \"job_id\": \"s.job_id\",\n",
    "            \"status\": \"s.status\",\n",
    "            \"start_time\": \"s.start_time\",\n",
    "            \"end_time\": \"s.end_time\",\n",
    "            \"backfill_job_id\": \"s.backfill_job_id\",\n",
    "            \"error_message\": \"s.error_message\"\n",
    "        }\n",
    "        \n",
    "        # Add optional columns only if they exist\n",
    "        if \"creator_user_name\" in existing_columns:\n",
    "            update_set[\"creator_user_name\"] = \"s.creator_user_name\"\n",
    "            insert_values[\"creator_user_name\"] = \"s.creator_user_name\"\n",
    "        \n",
    "        if \"run_page_url\" in existing_columns:\n",
    "            update_set[\"run_page_url\"] = \"s.run_page_url\"\n",
    "            insert_values[\"run_page_url\"] = \"s.run_page_url\"\n",
    "        \n",
    "        if \"retry_metadata\" in existing_columns:\n",
    "            update_set[\"retry_metadata\"] = \"s.retry_metadata\"\n",
    "            insert_values[\"retry_metadata\"] = \"s.retry_metadata\"\n",
    "        \n",
    "        # MERGE: Explicitly set all fields\n",
    "        (delta_table.alias(\"t\")\n",
    "            .merge(source_df.alias(\"s\"), \"t.run_id = s.run_id\")\n",
    "            .whenMatchedUpdate(set=update_set)\n",
    "            .whenNotMatchedInsert(values=insert_values)\n",
    "            .execute()\n",
    "        )\n",
    "        \n",
    "        return status\n",
    "    \n",
    "    def start(self):\n",
    "        \"\"\"Log the start of processing.\"\"\"\n",
    "        self._upsert(\"STARTED\")\n",
    "        print(f\"▶ Started: {self.run_id}\")\n",
    "        return self\n",
    "    \n",
    "    def success(self, backfill_job_id=None, creator_user_name=None, run_page_url=None):\n",
    "        \"\"\"Log successful completion.\"\"\"\n",
    "        self._upsert(\"SUCCESS\", backfill_job_id=backfill_job_id, creator_user_name=creator_user_name, run_page_url=run_page_url)\n",
    "        print(f\"✓ Success: {self.run_id}\")\n",
    "        return self\n",
    "    \n",
    "    def fail(self, error_message, backfill_job_id=None, creator_user_name=None, run_page_url=None):\n",
    "        \"\"\"Log failure with error message.\"\"\"\n",
    "        self._upsert(\"FAILED\", backfill_job_id=backfill_job_id, error_message=error_message, creator_user_name=creator_user_name, run_page_url=run_page_url)\n",
    "        print(f\"✗ Failed: {self.run_id} - {error_message}\")\n",
    "        return self\n",
    "    \n",
    "    def skip(self, reason, backfill_job_id=None, creator_user_name=None, run_page_url=None):\n",
    "        \"\"\"Log skip with reason.\"\"\"\n",
    "        self._upsert(\"SKIPPED\", backfill_job_id=backfill_job_id, error_message=reason, creator_user_name=creator_user_name, run_page_url=run_page_url)\n",
    "        print(f\"⊘ Skipped: {self.run_id} - {reason}\")\n",
    "        return self\n",
    "    \n",
    "    def update_backfill_id(self, backfill_job_id):\n",
    "        \"\"\"Update only the backfill job ID.\"\"\"\n",
    "        spark.sql(f\"\"\"\n",
    "            UPDATE {log_table}\n",
    "            SET backfill_job_id = '{backfill_job_id}'\n",
    "            WHERE run_id = '{self.run_id}'\n",
    "        \"\"\")\n",
    "        print(f\"↻ Updated backfill_job_id: {backfill_job_id}\")\n",
    "        return self\n",
    "\n",
    "print(\"✓ BackfillLogger class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "282c99f0-1499-40da-8cc1-f26a8e5c22e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 6: BackfillLogger Class\n",
    "\n",
    "This class provides a simple, intuitive interface for logging backfill operations to the log table.\n",
    "\n",
    "**Features:**\n",
    "- Logs START/SUCCESS/FAILED/SKIPPED status with timestamps\n",
    "- Captures job metadata (run ID, creator, job URL, error messages)\n",
    "- Supports retry metadata tracking (JSON column)\n",
    "- Backward compatible (works with/without retry_metadata column)\n",
    "- Uses Delta MERGE for upsert operations (handles reruns gracefully)\n",
    "\n",
    "**Usage:**\n",
    "```python\n",
    "logger = BackfillLogger(run_id, position_date, job_name, job_id, retry_metadata_json=None)\n",
    "logger.start()  # Log start\n",
    "logger.success()  # Or logger.fail(\"error\") or logger.skip(\"reason\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f4a3474-9e52-485f-b912-28a8f8661404",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Columns already exist in demos.backfill_demo.backfill_log\n"
     ]
    }
   ],
   "source": [
    "# Add new columns to existing table (run this once to update schema)\n",
    "try:\n",
    "    # Try to add all columns at once\n",
    "    spark.sql(f\"ALTER TABLE {log_table} ADD COLUMNS (creator_user_name STRING, run_page_url STRING, retry_metadata STRING COMMENT 'JSON: {{is_retry, original_run_id, retry_count, retry_triggered_by}}')\")\n",
    "    print(f\"✓ Added new columns to {log_table}\")\n",
    "except Exception as e:\n",
    "    if \"already exists\" in str(e).lower():\n",
    "        print(f\"✓ Columns already exist in {log_table}\")\n",
    "    else:\n",
    "        # Try adding columns individually\n",
    "        for col_name, col_def in [\n",
    "            (\"creator_user_name\", \"STRING\"),\n",
    "            (\"run_page_url\", \"STRING\"),\n",
    "            (\"retry_metadata\", \"STRING COMMENT 'JSON: {is_retry, original_run_id, retry_count, retry_triggered_by}'\")\n",
    "        ]:\n",
    "            try:\n",
    "                spark.sql(f\"ALTER TABLE {log_table} ADD COLUMNS ({col_name} {col_def})\")\n",
    "                print(f\"  ✓ Added column: {col_name}\")\n",
    "            except Exception as col_error:\n",
    "                if \"already exists\" in str(col_error).lower():\n",
    "                    print(f\"  ✓ Column exists: {col_name}\")\n",
    "                else:\n",
    "                    print(f\"  Note: {col_name} - {col_error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc507de3-c045-41ab-a820-c82eb1e6e387",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 7: Schema Migration (Run Once)\n",
    "\n",
    "This cell adds new columns to the backfill_log table if they don't exist:\n",
    "- `creator_user_name`: User who triggered the job\n",
    "- `run_page_url`: URL to the job run in Databricks UI\n",
    "- `retry_metadata`: JSON column for tracking retry information\n",
    "\n",
    "**Note:** This cell is backward compatible and safe to run multiple times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6dc663cf-5c30-41b5-99a2-639a35dc0798",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: Upserting - run_id=774114517055437, pos_date=2025-01-03, job_name=02_process_data, job_id=743256224103762, status=STARTED\n▶ Started: 774114517055437\n"
     ]
    }
   ],
   "source": [
    "# Create logger and start logging\n",
    "logger = BackfillLogger(\n",
    "    run_id=job_run_id,\n",
    "    position_date=position_date_str,\n",
    "    job_name=job_name,\n",
    "    job_id=job_id,\n",
    "    retry_metadata_json=retry_metadata_json\n",
    ").start()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d91b743b-6f79-4255-88ea-a6f1d593f062",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 8: Initialize Logger\n",
    "\n",
    "Create the BackfillLogger instance with the run context and start logging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c55e608-2345-447b-8e5e-b69479f58347",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "{'position_date': '2025-01-03',\n",
       " 'is_business_day': True,\n",
       " 'calendar_table': 'demos.backfill_demo.calendar'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import config to get calendar table\n",
    "import sys\n",
    "notebook_path = dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get()\n",
    "base_path = f\"/Workspace{notebook_path}\".rsplit('/', 1)[0]\n",
    "sys.path.append(base_path)\n",
    "from config import CALENDAR_TABLE\n",
    "\n",
    "# Check if position date is a business day\n",
    "query = f\"\"\"\n",
    "    SELECT COUNT(1) AS is_business_day\n",
    "    FROM {CALENDAR_TABLE}\n",
    "    WHERE us_business_or_holiday_flag = 'B'\n",
    "      AND calendar_date = '{position_date_str}'\n",
    "\"\"\"\n",
    "df = spark.sql(query)\n",
    "is_business_day = df.first()[\"is_business_day\"] > 0\n",
    "display({\"position_date\": position_date_str, \"is_business_day\": is_business_day, \"calendar_table\": CALENDAR_TABLE})\n",
    "\n",
    "if not is_business_day:\n",
    "    # Use the logger object (not the old log_status function)\n",
    "    logger.skip(\"Not a business day\")\n",
    "    dbutils.notebook.exit(\"Notebook exited: Not a business day\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93f18a3e-46ab-4400-a48d-28761f520c77",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 9: Business Day Validation\n",
    "\n",
    "Query the calendar table to check if the position_date is a business day:\n",
    "- **Business Day (B)**: Proceed with processing\n",
    "- **Holiday/Weekend (H)**: Skip with logging and exit notebook\n",
    "\n",
    "This prevents unnecessary job runs for non-business dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8baec04a-5f03-4453-ae09-31b286a32c4e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Triggering job 743256224103762 for position_date: 2025-01-03\nJob triggered successfully. Run ID: 923018509174690\n↻ Updated backfill_job_id: 923018509174690\nMonitoring job execution...\n  Status: RunLifeCycleState.RUNNING | Result: PENDING\n  Status: RunLifeCycleState.RUNNING | Result: PENDING\n  Status: RunLifeCycleState.RUNNING | Result: PENDING\n  Status: RunLifeCycleState.RUNNING | Result: PENDING\n  Status: RunLifeCycleState.TERMINATED | Result: RunResultState.SUCCESS\n\nJob completed with state: RunLifeCycleState.TERMINATED, result: RunResultState.SUCCESS\nDEBUG: Upserting - run_id=774114517055437, pos_date=2025-01-03, job_name=02_process_data, job_id=743256224103762, status=SUCCESS\n✓ Success: 774114517055437\n"
     ]
    }
   ],
   "source": [
    "from databricks.sdk import WorkspaceClient\n",
    "import time\n",
    "\n",
    "w = WorkspaceClient()\n",
    "\n",
    "try:\n",
    "    # Trigger the job with the position_date from widget\n",
    "    print(f\"Triggering job {job_id} for position_date: {position_date_str}\")\n",
    "    response = w.jobs.run_now(job_id=job_id, notebook_params={\"position_date\": position_date_str})\n",
    "    triggered_run_id = response.run_id\n",
    "    \n",
    "    print(f\"Job triggered successfully. Run ID: {triggered_run_id}\")\n",
    "    \n",
    "    # Update logger with the triggered run ID\n",
    "    logger.update_backfill_id(str(triggered_run_id))\n",
    "    \n",
    "    # Poll for job completion\n",
    "    print(\"Monitoring job execution...\")\n",
    "    while True:\n",
    "        run_status = w.jobs.get_run(run_id=triggered_run_id)\n",
    "        life_cycle_state = str(run_status.state.life_cycle_state)\n",
    "        result_state = str(run_status.state.result_state) if run_status.state.result_state else \"PENDING\"\n",
    "        \n",
    "        print(f\"  Status: {life_cycle_state} | Result: {result_state}\")\n",
    "        \n",
    "        # Check if job is in terminal state\n",
    "        if \"TERMINATED\" in life_cycle_state or \"SKIPPED\" in life_cycle_state or \"INTERNAL_ERROR\" in life_cycle_state:\n",
    "            print(f\"\\nJob completed with state: {life_cycle_state}, result: {result_state}\")\n",
    "            \n",
    "            # Extract creator and run page URL\n",
    "            creator_user = run_status.creator_user_name if hasattr(run_status, 'creator_user_name') else None\n",
    "            run_url = run_status.run_page_url if hasattr(run_status, 'run_page_url') else None\n",
    "            \n",
    "            # Update logger based on result\n",
    "            if \"SUCCESS\" in result_state:\n",
    "                logger.success(\n",
    "                    backfill_job_id=str(triggered_run_id),\n",
    "                    creator_user_name=creator_user,\n",
    "                    run_page_url=run_url\n",
    "                )\n",
    "            else:\n",
    "                error_msg = run_status.state.state_message if run_status.state.state_message else f\"Job failed with state: {life_cycle_state}, result: {result_state}\"\n",
    "                logger.fail(\n",
    "                    error_msg,\n",
    "                    backfill_job_id=str(triggered_run_id),\n",
    "                    creator_user_name=creator_user,\n",
    "                    run_page_url=run_url\n",
    "                )\n",
    "            \n",
    "            break\n",
    "        \n",
    "        time.sleep(10)\n",
    "    \n",
    "    # display(run_status.as_dict())\n",
    "\n",
    "except Exception as e:\n",
    "    error_message = f\"Failed to trigger or monitor job: {str(e)}\"\n",
    "    print(f\"✗ {error_message}\")\n",
    "    logger.fail(error_message)\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e39f31d8-5794-4c37-b721-5a38dc861ac5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 10: Trigger and Monitor Job\n",
    "\n",
    "This cell performs the core orchestration:\n",
    "\n",
    "1. **Trigger Job**: Uses Databricks SDK to trigger the processing job with position_date parameter\n",
    "2. **Monitor Execution**: Polls job status every 10 seconds until completion\n",
    "3. **Capture Results**: Extracts creator, run URL, and error messages\n",
    "4. **Update Logger**: Logs final status (SUCCESS or FAILED) with all metadata\n",
    "\n",
    "**Job States:**\n",
    "- **PENDING/RUNNING**: Job is executing\n",
    "- **TERMINATED + SUCCESS**: Job completed successfully\n",
    "- **TERMINATED + FAILED**: Job encountered an error\n",
    "- **SKIPPED**: Job was skipped\n",
    "- **INTERNAL_ERROR**: Databricks internal error"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "03_backfill_orchestrator",
   "widgets": {
    "job_name": {
     "currentValue": "02_process_data",
     "nuid": "8e348a9d-6a26-45a2-99e2-3969e42562fe",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "Job Name",
      "name": "job_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "Job Name",
      "name": "job_name",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "position_date": {
     "currentValue": "2025-01-03",
     "nuid": "d227e8a8-7a2e-4f3f-b0d4-1bba2bf99ba1",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "Position Date (YYYY-MM-DD)",
      "name": "position_date",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "Position Date (YYYY-MM-DD)",
      "name": "position_date",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "retry_metadata": {
     "currentValue": "",
     "nuid": "e8219925-5008-4179-9b11-0ebf46a249d4",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "Retry Metadata (JSON)",
      "name": "retry_metadata",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "Retry Metadata (JSON)",
      "name": "retry_metadata",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}